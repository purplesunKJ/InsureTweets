{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T07:47:10.732995Z",
     "start_time": "2018-12-25T07:47:10.398854Z"
    }
   },
   "outputs": [],
   "source": [
    "## Import Libraries and Dependencies ##\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defined Emoticons Symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T07:47:10.746930Z",
     "start_time": "2018-12-25T07:47:10.733958Z"
    }
   },
   "outputs": [],
   "source": [
    "# Happy Emoticons\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    " \n",
    "# Sad Emoticons\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    "\n",
    "# all emoticons (happy + sad)\n",
    "emoticons = emoticons_happy.union(emoticons_sad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T07:47:10.757889Z",
     "start_time": "2018-12-25T07:47:10.748941Z"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_PATH = \"D:/OneDrive - National University of Singapore/NUS MTech KE/MTech KE - FYP - InsureSense/Kang Jiang/Phase 3/System Implementation/scripts/Data Mining & Machine Learning/dataset/disaster_phase_text_classification/model/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T07:47:10.764883Z",
     "start_time": "2018-12-25T07:47:10.760882Z"
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = \"D:/OneDrive - National University of Singapore/NUS MTech KE/MTech KE - FYP - InsureSense/Kang Jiang/Phase 3/System Implementation/scripts/Data Mining & Machine Learning/dataset/sentiment_analysis/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T07:47:11.021217Z",
     "start_time": "2018-12-25T07:47:10.766869Z"
    }
   },
   "outputs": [],
   "source": [
    "df_Japan_Floods_full = pd.read_csv(INPUT_PATH + \"df_Japan_floods_phase_labelled.csv\", index_col=0, encoding = \"ISO-8859-1\")\n",
    "# df_Typhoon_Jebi_full = pd.read_csv(INPUT_PATH + 'df_Typhoon_Jebi_phase_labelled.csv', index_col=0)\n",
    "# df_Typhoon_Mangkhut_full = pd.read_csv(INPUT_PATH + 'df_Typhoon_Mangkhut_phase_labelled.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T07:47:11.026180Z",
     "start_time": "2018-12-25T07:47:11.023179Z"
    }
   },
   "outputs": [],
   "source": [
    "df_disaster_full = df_Japan_Floods_full\n",
    "df_disaster_full = df_disaster_full[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T07:47:11.051107Z",
     "start_time": "2018-12-25T07:47:11.028170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10 entries, 0 to 9\n",
      "Data columns (total 13 columns):\n",
      "tweet_id          10 non-null float64\n",
      "user              10 non-null object\n",
      "timestamp         10 non-null object\n",
      "date              10 non-null object\n",
      "events            10 non-null object\n",
      "text              10 non-null object\n",
      "processed_text    10 non-null object\n",
      "likes             10 non-null int64\n",
      "replies           10 non-null int64\n",
      "retweets          10 non-null int64\n",
      "url               10 non-null object\n",
      "disaster_flag     10 non-null int64\n",
      "disaster_phase    10 non-null int64\n",
      "dtypes: float64(1), int64(5), object(7)\n",
      "memory usage: 1.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df_disaster_full.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T07:47:11.072060Z",
     "start_time": "2018-12-25T07:47:11.053099Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>user</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>date</th>\n",
       "      <th>events</th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>likes</th>\n",
       "      <th>replies</th>\n",
       "      <th>retweets</th>\n",
       "      <th>url</th>\n",
       "      <th>disaster_flag</th>\n",
       "      <th>disaster_phase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.005360e+18</td>\n",
       "      <td>@jjwalsh</td>\n",
       "      <td>6/9/2018 7:47</td>\n",
       "      <td>6/9/2018</td>\n",
       "      <td>Japan Floods</td>\n",
       "      <td>Nice day by the river in #Kobe on this beautif...</td>\n",
       "      <td>nice day river kobe beautiful sunny weather sa...</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>/jjwalsh/status/1005355731665629184</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.005370e+18</td>\n",
       "      <td>@metalheadbazaar</td>\n",
       "      <td>6/9/2018 8:29</td>\n",
       "      <td>6/9/2018</td>\n",
       "      <td>Japan Floods</td>\n",
       "      <td>Marduk - To Tour Japan In November - Metal Sto...</td>\n",
       "      <td>marduk tour japan november metal storm</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/metalheadbazaar/status/1005366203618156546</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.005370e+18</td>\n",
       "      <td>@wordwidetroll</td>\n",
       "      <td>6/9/2018 8:45</td>\n",
       "      <td>6/9/2018</td>\n",
       "      <td>Japan Floods</td>\n",
       "      <td>Yestarday storm give me http://www.irvinakatec...</td>\n",
       "      <td>yestarday storm give ad adsense money moneygur...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/wordwidetroll/status/1005370364757725184</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.005370e+18</td>\n",
       "      <td>@kazuotamakashi</td>\n",
       "      <td>6/9/2018 9:00</td>\n",
       "      <td>6/9/2018</td>\n",
       "      <td>Japan Floods</td>\n",
       "      <td>Rain tomorrow, at Nara City, Japan! With a hig...</td>\n",
       "      <td>rain tomorrow nara city japan high 22c low 18c</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/kazuotamakashi/status/1005373968650571778</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.005370e+18</td>\n",
       "      <td>@shukyudo_travel</td>\n",
       "      <td>6/9/2018 9:00</td>\n",
       "      <td>6/9/2018</td>\n",
       "      <td>Japan Floods</td>\n",
       "      <td>Rain tomorrow! With a high of 79F and a low of...</td>\n",
       "      <td>rain tomorrow high 79f low 70f japan osaka tra...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/shukyudo_travel/status/1005373973142622210</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tweet_id              user      timestamp      date        events  \\\n",
       "0  1.005360e+18          @jjwalsh  6/9/2018 7:47  6/9/2018  Japan Floods   \n",
       "1  1.005370e+18  @metalheadbazaar  6/9/2018 8:29  6/9/2018  Japan Floods   \n",
       "2  1.005370e+18    @wordwidetroll  6/9/2018 8:45  6/9/2018  Japan Floods   \n",
       "3  1.005370e+18   @kazuotamakashi  6/9/2018 9:00  6/9/2018  Japan Floods   \n",
       "4  1.005370e+18  @shukyudo_travel  6/9/2018 9:00  6/9/2018  Japan Floods   \n",
       "\n",
       "                                                text  \\\n",
       "0  Nice day by the river in #Kobe on this beautif...   \n",
       "1  Marduk - To Tour Japan In November - Metal Sto...   \n",
       "2  Yestarday storm give me http://www.irvinakatec...   \n",
       "3  Rain tomorrow, at Nara City, Japan! With a hig...   \n",
       "4  Rain tomorrow! With a high of 79F and a low of...   \n",
       "\n",
       "                                      processed_text  likes  replies  \\\n",
       "0  nice day river kobe beautiful sunny weather sa...     26        3   \n",
       "1             marduk tour japan november metal storm      0        0   \n",
       "2  yestarday storm give ad adsense money moneygur...      0        0   \n",
       "3     rain tomorrow nara city japan high 22c low 18c      0        0   \n",
       "4  rain tomorrow high 79f low 70f japan osaka tra...      0        0   \n",
       "\n",
       "   retweets                                          url  disaster_flag  \\\n",
       "0         3          /jjwalsh/status/1005355731665629184              1   \n",
       "1         0  /metalheadbazaar/status/1005366203618156546              1   \n",
       "2         0    /wordwidetroll/status/1005370364757725184              1   \n",
       "3         0   /kazuotamakashi/status/1005373968650571778              1   \n",
       "4         0  /shukyudo_travel/status/1005373973142622210              1   \n",
       "\n",
       "   disaster_phase  \n",
       "0               3  \n",
       "1               3  \n",
       "2               3  \n",
       "3               1  \n",
       "4               1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_disaster_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Score Labelling - Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T07:47:11.720634Z",
     "start_time": "2018-12-25T07:47:11.074048Z"
    }
   },
   "outputs": [],
   "source": [
    "## Import Libraries and Dependencies ##\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "stopwords_english = stopwords.words('english')\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T07:47:11.758500Z",
     "start_time": "2018-12-25T07:47:11.721599Z"
    }
   },
   "outputs": [],
   "source": [
    "# tweet cleaning function\n",
    "def clean_tweets_1(tweet):\n",
    "\n",
    "    wnlemma = nltk.WordNetLemmatizer()\n",
    "    printable = set(string.printable)\n",
    "\n",
    "    # remove non ASCII word\n",
    "    tweet = ''.join(filter(lambda x: x in printable, tweet))\n",
    "    \n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    \n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    \n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    \n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "\n",
    "    tweet = tweet.strip()\n",
    "    \n",
    "    tweet = re.sub(' +',' ', tweet)\n",
    "    \n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    " \n",
    "    tweets_clean = []    \n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and # remove stopwords\n",
    "              word not in emoticons and # remove emoticons\n",
    "                word not in string.punctuation): # remove punctuation\n",
    "            lemma_word = wnlemma.lemmatize(word)\n",
    "            tweets_clean.append(lemma_word)\n",
    "            \n",
    "#     tweets_clean = \" \".join(tweets_clean)\n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T07:47:11.766478Z",
     "start_time": "2018-12-25T07:47:11.760518Z"
    }
   },
   "outputs": [],
   "source": [
    "# feature extractor function\n",
    "def bag_of_words(tweet):\n",
    "    words = clean_tweets_1(tweet)\n",
    "    words_dictionary = dict([word, True] for word in words)    \n",
    "    return words_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-22T16:28:45.250776Z",
     "start_time": "2018-12-22T16:28:45.235817Z"
    }
   },
   "source": [
    "## Sentiment model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T07:47:12.162459Z",
     "start_time": "2018-12-25T07:47:11.767478Z"
    }
   },
   "outputs": [],
   "source": [
    "pos_tweets = twitter_samples.strings('positive_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T07:47:12.526448Z",
     "start_time": "2018-12-25T07:47:12.163415Z"
    }
   },
   "outputs": [],
   "source": [
    "neg_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T07:47:14.467374Z",
     "start_time": "2018-12-25T07:47:12.528440Z"
    }
   },
   "outputs": [],
   "source": [
    "# positive tweets feature set\n",
    "pos_tweets_set = []\n",
    "for tweet in pos_tweets:\n",
    "    pos_tweets_set.append((bag_of_words(tweet), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T07:47:15.051846Z",
     "start_time": "2018-12-25T07:47:14.468375Z"
    }
   },
   "outputs": [],
   "source": [
    "# negative tweets feature set\n",
    "neg_tweets_set = []\n",
    "for tweet in neg_tweets:\n",
    "    neg_tweets_set.append((bag_of_words(tweet), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T07:47:15.057798Z",
     "start_time": "2018-12-25T07:47:15.053839Z"
    }
   },
   "outputs": [],
   "source": [
    "full_set = pos_tweets_set + neg_tweets_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment model classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T07:47:15.066803Z",
     "start_time": "2018-12-25T07:47:15.059793Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T07:47:15.292195Z",
     "start_time": "2018-12-25T07:47:15.068769Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train the sentiment model using open source training dataset\n",
    "classifier = NaiveBayesClassifier.train(full_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T07:47:16.685483Z",
     "start_time": "2018-12-25T07:47:15.294166Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\purpl\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\purpl\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:194: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "C:\\Users\\purpl\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Classify disaster tweets into positive or negative tweets\n",
    "df_disaster_full['sentiment_1'] = \"\"\n",
    "for index, row in df_disaster_full.iterrows():\n",
    "    \n",
    "    tweet_text_set = bag_of_words(row['text'])\n",
    "    df_disaster_full['sentiment_1'].iloc[index] = classifier.classify(tweet_text_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Score Labelling - Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T07:47:16.717359Z",
     "start_time": "2018-12-25T07:47:16.686440Z"
    }
   },
   "outputs": [],
   "source": [
    "# tweet cleaning function\n",
    "def clean_tweets_2(tweet):\n",
    "    \n",
    "    wnlemma = nltk.WordNetLemmatizer()\n",
    "    printable = set(string.printable)\n",
    "\n",
    "    # remove non ASCII word\n",
    "    tweet = ''.join(filter(lambda x: x in printable, tweet))\n",
    "    \n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    \n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    \n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    \n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "\n",
    "    tweet = tweet.strip()\n",
    "    \n",
    "    tweet = re.sub(' +',' ', tweet)\n",
    "    \n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    " \n",
    "    tweets_clean = []    \n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and # remove stopwords\n",
    "              word not in emoticons and # remove emoticons\n",
    "                word not in string.punctuation): # remove punctuation\n",
    "            lemma_word = wnlemma.lemmatize(word)\n",
    "            tweets_clean.append(lemma_word)\n",
    "            \n",
    "    tweets_clean = \" \".join(tweets_clean)\n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment polarity score labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T07:47:18.102693Z",
     "start_time": "2018-12-25T07:47:16.718358Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\purpl\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\purpl\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:194: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "C:\\Users\\purpl\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "df_disaster_full['processed_text'] = \"\"\n",
    "for index, row in df_disaster_full.iterrows():\n",
    "    df_disaster_full['processed_text'].iloc[index] = clean_tweets_2(row['text'])\n",
    "df_disaster_full = df_disaster_full[['tweet_id','user','timestamp','date','events','text','processed_text','likes','replies','retweets','url','disaster_flag','disaster_phase','sentiment_1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T07:47:18.854689Z",
     "start_time": "2018-12-25T07:47:18.103650Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\purpl\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:194: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "# Import the library for sentiment analysis\n",
    "from textblob import TextBlob\n",
    "\n",
    "# For each of the comment, calculate the sentiment polarity and store in a new collumn as sentiment polarity score\n",
    "df_disaster_full['sentiment_2'] = \"\"\n",
    "for index, row in df_disaster_full.iterrows():\n",
    "    blob = TextBlob(df_disaster_full['processed_text'].iloc[index])\n",
    "    for sentence in blob.sentences:\n",
    "        df_disaster_full['sentiment_2'].iloc[index] = sentence.sentiment.polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Score Labelling -  Method 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T07:47:19.052172Z",
     "start_time": "2018-12-25T07:47:18.855659Z"
    }
   },
   "outputs": [],
   "source": [
    "import json, requests\n",
    "class StanfordCoreNLP:\n",
    "    \"\"\"\n",
    "    Modified from https://github.com/smilli/py-corenlp\n",
    "    \"\"\"\n",
    " \n",
    "    def __init__(self, server_url):\n",
    "        # TODO: Error handling? More checking on the url?\n",
    "        if server_url[-1] == '/':\n",
    "            server_url = server_url[:-1]\n",
    "        self.server_url = server_url\n",
    " \n",
    "    def annotate(self, text, properties=None):\n",
    "        assert isinstance(text, str)\n",
    "        if properties is None:\n",
    "            properties = {}\n",
    "        else:\n",
    "            assert isinstance(properties, dict)\n",
    " \n",
    "        # Checks that the Stanford CoreNLP server is started.\n",
    "        try:\n",
    "            requests.get(self.server_url)\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            raise Exception('Check whether you have started the CoreNLP server e.g.\\n'\n",
    "                            '$ cd <path_to_core_nlp_folder>/stanford-corenlp-full-2016-10-31/ \\n'\n",
    "                            '$ java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port <port> -timeout <timeout_in_ms>')\n",
    " \n",
    "        data = text.encode()\n",
    "        r = requests.post(\n",
    "            self.server_url, params={\n",
    "                'properties': str(properties)\n",
    "            }, data=data, headers={'Connection': 'close'})\n",
    "        output = r.text\n",
    "        if ('outputFormat' in properties\n",
    "            and properties['outputFormat'] == 'json'):\n",
    "            try:\n",
    "                output = json.loads(output, encoding='utf-8', strict=True)\n",
    "            except:\n",
    "                pass\n",
    "        return output\n",
    "\n",
    "    \n",
    "def sentiment_analysis_on_sentence(sentence):\n",
    "    # The StanfordCoreNLP server is running on http://127.0.0.1:9000\n",
    "    nlp = StanfordCoreNLP('http://127.0.0.1:9000')\n",
    "    # Json response of all the annotations\n",
    "    output = nlp.annotate(sentence, properties={\n",
    "        \"annotators\": \"tokenize,ssplit,parse,sentiment\",\n",
    "        \"outputFormat\": \"json\",\n",
    "        # Only split the sentence at End Of Line. We assume that this method only takes in one single sentence.\n",
    "        \"ssplit.eolonly\": \"true\",\n",
    "        # Setting enforceRequirements to skip some annotators and make the process faster\n",
    "        \"enforceRequirements\": \"false\"\n",
    "    })\n",
    "    # Only care about the result of the first sentence because we assume we only annotate a single sentence in this method.\n",
    "    return int(output['sentences'][0]['sentimentValue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T07:47:19.058117Z",
     "start_time": "2018-12-25T07:47:19.054126Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_disaster_full['sentiment_3'] = \"\"\n",
    "\n",
    "# for index, row in df_disaster_full.iterrows():\n",
    "#     df_disaster_full['processed_text'].iloc[index] = clean_tweets_2(row['text'])\n",
    "#     df_disaster_full['sentiment_3'].iloc[index] = sentiment_analysis_on_sentence(row['processed_text'])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Sentiment Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T07:47:19.079060Z",
     "start_time": "2018-12-25T07:47:19.060113Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>user</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>date</th>\n",
       "      <th>events</th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>likes</th>\n",
       "      <th>replies</th>\n",
       "      <th>retweets</th>\n",
       "      <th>url</th>\n",
       "      <th>disaster_flag</th>\n",
       "      <th>disaster_phase</th>\n",
       "      <th>sentiment_1</th>\n",
       "      <th>sentiment_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.005360e+18</td>\n",
       "      <td>@jjwalsh</td>\n",
       "      <td>6/9/2018 7:47</td>\n",
       "      <td>6/9/2018</td>\n",
       "      <td>Japan Floods</td>\n",
       "      <td>Nice day by the river in #Kobe on this beautif...</td>\n",
       "      <td>nice day river kobe beautiful sunny weather sa...</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>/jjwalsh/status/1005355731665629184</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.683333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.005370e+18</td>\n",
       "      <td>@metalheadbazaar</td>\n",
       "      <td>6/9/2018 8:29</td>\n",
       "      <td>6/9/2018</td>\n",
       "      <td>Japan Floods</td>\n",
       "      <td>Marduk - To Tour Japan In November - Metal Sto...</td>\n",
       "      <td>marduk tour japan november metal storm</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/metalheadbazaar/status/1005366203618156546</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.005370e+18</td>\n",
       "      <td>@wordwidetroll</td>\n",
       "      <td>6/9/2018 8:45</td>\n",
       "      <td>6/9/2018</td>\n",
       "      <td>Japan Floods</td>\n",
       "      <td>Yestarday storm give me http://www.irvinakatec...</td>\n",
       "      <td>yestarday storm give ad adsense money moneygur...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/wordwidetroll/status/1005370364757725184</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.005370e+18</td>\n",
       "      <td>@kazuotamakashi</td>\n",
       "      <td>6/9/2018 9:00</td>\n",
       "      <td>6/9/2018</td>\n",
       "      <td>Japan Floods</td>\n",
       "      <td>Rain tomorrow, at Nara City, Japan! With a hig...</td>\n",
       "      <td>rain tomorrow nara city japan high 22c low 18c</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/kazuotamakashi/status/1005373968650571778</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.005370e+18</td>\n",
       "      <td>@shukyudo_travel</td>\n",
       "      <td>6/9/2018 9:00</td>\n",
       "      <td>6/9/2018</td>\n",
       "      <td>Japan Floods</td>\n",
       "      <td>Rain tomorrow! With a high of 79F and a low of...</td>\n",
       "      <td>rain tomorrow high 79f low 70f japan osaka tra...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/shukyudo_travel/status/1005373973142622210</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tweet_id              user      timestamp      date        events  \\\n",
       "0  1.005360e+18          @jjwalsh  6/9/2018 7:47  6/9/2018  Japan Floods   \n",
       "1  1.005370e+18  @metalheadbazaar  6/9/2018 8:29  6/9/2018  Japan Floods   \n",
       "2  1.005370e+18    @wordwidetroll  6/9/2018 8:45  6/9/2018  Japan Floods   \n",
       "3  1.005370e+18   @kazuotamakashi  6/9/2018 9:00  6/9/2018  Japan Floods   \n",
       "4  1.005370e+18  @shukyudo_travel  6/9/2018 9:00  6/9/2018  Japan Floods   \n",
       "\n",
       "                                                text  \\\n",
       "0  Nice day by the river in #Kobe on this beautif...   \n",
       "1  Marduk - To Tour Japan In November - Metal Sto...   \n",
       "2  Yestarday storm give me http://www.irvinakatec...   \n",
       "3  Rain tomorrow, at Nara City, Japan! With a hig...   \n",
       "4  Rain tomorrow! With a high of 79F and a low of...   \n",
       "\n",
       "                                      processed_text  likes  replies  \\\n",
       "0  nice day river kobe beautiful sunny weather sa...     26        3   \n",
       "1             marduk tour japan november metal storm      0        0   \n",
       "2  yestarday storm give ad adsense money moneygur...      0        0   \n",
       "3     rain tomorrow nara city japan high 22c low 18c      0        0   \n",
       "4  rain tomorrow high 79f low 70f japan osaka tra...      0        0   \n",
       "\n",
       "   retweets                                          url  disaster_flag  \\\n",
       "0         3          /jjwalsh/status/1005355731665629184              1   \n",
       "1         0  /metalheadbazaar/status/1005366203618156546              1   \n",
       "2         0    /wordwidetroll/status/1005370364757725184              1   \n",
       "3         0   /kazuotamakashi/status/1005373968650571778              1   \n",
       "4         0  /shukyudo_travel/status/1005373973142622210              1   \n",
       "\n",
       "   disaster_phase  sentiment_1 sentiment_2  \n",
       "0               3            1    0.683333  \n",
       "1               3           -1           0  \n",
       "2               3           -1           0  \n",
       "3               1           -1        0.08  \n",
       "4               1           -1        0.08  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_disaster_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T07:47:19.088070Z",
     "start_time": "2018-12-25T07:47:19.081055Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract columns\n",
    "df_disaster_full = df_disaster_full[['tweet_id','user','timestamp','date','events','text', 'processed_text','likes','replies','retweets','url','disaster_flag','disaster_phase','sentiment_1','sentiment_2']]\n",
    "# df_disaster_full = df_disaster_full[['tweet_id','user','timestamp','date','events','text', 'processed_text','likes','replies','retweets','url','disaster_flag','disaster_phase','sentiment_1','sentiment_2','sentiment_3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T07:47:19.100005Z",
     "start_time": "2018-12-25T07:47:19.090033Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert sentiment score from method 1 and method 2 to float\n",
    "df_disaster_full['sentiment_1'] = df_disaster_full['sentiment_1'].astype(float)\n",
    "df_disaster_full['sentiment_2'] = df_disaster_full['sentiment_2'].astype(float)\n",
    "# df_disaster_full['sentiment_3'] = df_disaster_full['sentiment_3'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T07:47:19.166910Z",
     "start_time": "2018-12-25T07:47:19.101012Z"
    }
   },
   "outputs": [],
   "source": [
    "# Normalize sentiment score from method 1 and method 2 to [-1,1]\n",
    "df_disaster_full['sentiment_1_norm'] = \\\n",
    "    2*((df_disaster_full['sentiment_1']-df_disaster_full['sentiment_1'].min())/(df_disaster_full['sentiment_1'].max()-df_disaster_full['sentiment_1'].min())) - 1\n",
    "\n",
    "df_disaster_full['sentiment_2_norm'] = \\\n",
    "    2*((df_disaster_full['sentiment_2']-df_disaster_full['sentiment_2'].min())/(df_disaster_full['sentiment_2'].max()-df_disaster_full['sentiment_2'].min())) - 1\n",
    "\n",
    "# df_disaster_full['sentiment_3_norm'] = \\\n",
    "#     2*((df_disaster_full['sentiment_3']-df_disaster_full['sentiment_3'].min())/(df_disaster_full['sentiment_3'].max()-df_disaster_full['sentiment_3'].min())) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T07:47:19.172817Z",
     "start_time": "2018-12-25T07:47:19.167850Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate final sentiment score by taking teh average of sentiment score from method 1 and 2\n",
    "df_disaster_full['sentiment_final'] = (df_disaster_full['sentiment_1_norm'] + df_disaster_full['sentiment_2_norm'])/2\n",
    "# df_disaster_full['sentiment_final'] = (df_disaster_full['sentiment_1_norm'] + df_disaster_full['sentiment_2_norm'] + df_disaster_full['sentiment_3_norm'])/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T07:47:19.197743Z",
     "start_time": "2018-12-25T07:47:19.174805Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>user</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>date</th>\n",
       "      <th>events</th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>likes</th>\n",
       "      <th>replies</th>\n",
       "      <th>retweets</th>\n",
       "      <th>url</th>\n",
       "      <th>disaster_flag</th>\n",
       "      <th>disaster_phase</th>\n",
       "      <th>sentiment_1</th>\n",
       "      <th>sentiment_2</th>\n",
       "      <th>sentiment_1_norm</th>\n",
       "      <th>sentiment_2_norm</th>\n",
       "      <th>sentiment_final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.005360e+18</td>\n",
       "      <td>@jjwalsh</td>\n",
       "      <td>6/9/2018 7:47</td>\n",
       "      <td>6/9/2018</td>\n",
       "      <td>Japan Floods</td>\n",
       "      <td>Nice day by the river in #Kobe on this beautif...</td>\n",
       "      <td>nice day river kobe beautiful sunny weather sa...</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>/jjwalsh/status/1005355731665629184</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.005370e+18</td>\n",
       "      <td>@metalheadbazaar</td>\n",
       "      <td>6/9/2018 8:29</td>\n",
       "      <td>6/9/2018</td>\n",
       "      <td>Japan Floods</td>\n",
       "      <td>Marduk - To Tour Japan In November - Metal Sto...</td>\n",
       "      <td>marduk tour japan november metal storm</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/metalheadbazaar/status/1005366203618156546</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.005370e+18</td>\n",
       "      <td>@wordwidetroll</td>\n",
       "      <td>6/9/2018 8:45</td>\n",
       "      <td>6/9/2018</td>\n",
       "      <td>Japan Floods</td>\n",
       "      <td>Yestarday storm give me http://www.irvinakatec...</td>\n",
       "      <td>yestarday storm give ad adsense money moneygur...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/wordwidetroll/status/1005370364757725184</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.005370e+18</td>\n",
       "      <td>@kazuotamakashi</td>\n",
       "      <td>6/9/2018 9:00</td>\n",
       "      <td>6/9/2018</td>\n",
       "      <td>Japan Floods</td>\n",
       "      <td>Rain tomorrow, at Nara City, Japan! With a hig...</td>\n",
       "      <td>rain tomorrow nara city japan high 22c low 18c</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/kazuotamakashi/status/1005373968650571778</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.765854</td>\n",
       "      <td>-0.882927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.005370e+18</td>\n",
       "      <td>@shukyudo_travel</td>\n",
       "      <td>6/9/2018 9:00</td>\n",
       "      <td>6/9/2018</td>\n",
       "      <td>Japan Floods</td>\n",
       "      <td>Rain tomorrow! With a high of 79F and a low of...</td>\n",
       "      <td>rain tomorrow high 79f low 70f japan osaka tra...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/shukyudo_travel/status/1005373973142622210</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.765854</td>\n",
       "      <td>-0.882927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tweet_id              user      timestamp      date        events  \\\n",
       "0  1.005360e+18          @jjwalsh  6/9/2018 7:47  6/9/2018  Japan Floods   \n",
       "1  1.005370e+18  @metalheadbazaar  6/9/2018 8:29  6/9/2018  Japan Floods   \n",
       "2  1.005370e+18    @wordwidetroll  6/9/2018 8:45  6/9/2018  Japan Floods   \n",
       "3  1.005370e+18   @kazuotamakashi  6/9/2018 9:00  6/9/2018  Japan Floods   \n",
       "4  1.005370e+18  @shukyudo_travel  6/9/2018 9:00  6/9/2018  Japan Floods   \n",
       "\n",
       "                                                text  \\\n",
       "0  Nice day by the river in #Kobe on this beautif...   \n",
       "1  Marduk - To Tour Japan In November - Metal Sto...   \n",
       "2  Yestarday storm give me http://www.irvinakatec...   \n",
       "3  Rain tomorrow, at Nara City, Japan! With a hig...   \n",
       "4  Rain tomorrow! With a high of 79F and a low of...   \n",
       "\n",
       "                                      processed_text  likes  replies  \\\n",
       "0  nice day river kobe beautiful sunny weather sa...     26        3   \n",
       "1             marduk tour japan november metal storm      0        0   \n",
       "2  yestarday storm give ad adsense money moneygur...      0        0   \n",
       "3     rain tomorrow nara city japan high 22c low 18c      0        0   \n",
       "4  rain tomorrow high 79f low 70f japan osaka tra...      0        0   \n",
       "\n",
       "   retweets                                          url  disaster_flag  \\\n",
       "0         3          /jjwalsh/status/1005355731665629184              1   \n",
       "1         0  /metalheadbazaar/status/1005366203618156546              1   \n",
       "2         0    /wordwidetroll/status/1005370364757725184              1   \n",
       "3         0   /kazuotamakashi/status/1005373968650571778              1   \n",
       "4         0  /shukyudo_travel/status/1005373973142622210              1   \n",
       "\n",
       "   disaster_phase  sentiment_1  sentiment_2  sentiment_1_norm  \\\n",
       "0               3          1.0     0.683333               1.0   \n",
       "1               3         -1.0     0.000000              -1.0   \n",
       "2               3         -1.0     0.000000              -1.0   \n",
       "3               1         -1.0     0.080000              -1.0   \n",
       "4               1         -1.0     0.080000              -1.0   \n",
       "\n",
       "   sentiment_2_norm  sentiment_final  \n",
       "0          1.000000         1.000000  \n",
       "1         -1.000000        -1.000000  \n",
       "2         -1.000000        -1.000000  \n",
       "3         -0.765854        -0.882927  \n",
       "4         -0.765854        -0.882927  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_disaster_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T07:47:19.208714Z",
     "start_time": "2018-12-25T07:47:19.200737Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove unused column\n",
    "df_disaster_full = df_disaster_full.drop(columns=['sentiment_1', 'sentiment_2', 'sentiment_1_norm', 'sentiment_2_norm'])\n",
    "# df_disaster_full = df_disaster_full.drop(columns=['sentiment_1', 'sentiment_2', 'sentiment_3', 'sentiment_1_norm', 'sentiment_2_norm', 'sentiment_3_norm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T07:47:19.214726Z",
     "start_time": "2018-12-25T07:47:19.210713Z"
    }
   },
   "outputs": [],
   "source": [
    "# Store the sentiment label dataset\n",
    "# df_disaster_full.to_csv(OUTPUT_PATH + \"df_Typhoon_Jebi_full_sentiment_labelled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
