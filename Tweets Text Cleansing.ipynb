{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# import nltk libraries for text mining\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "## Sentiment\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "stopwords_english = stopwords.words('english')\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"D:\\\\OneDrive - National University of Singapore\\\\NUS MTech KE\\\\MTech KE - FYP - InsureSense\\\\Kang Jiang\\\\System Implementation\\\\Data Acquisition and Storage\\\\Twitter\\\\output\\\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defined Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Happy Emoticons\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    " \n",
    "# Sad Emoticons\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    "\n",
    "# all emoticons (happy + sad)\n",
    "emoticons = emoticons_happy.union(emoticons_sad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(folder_name):\n",
    "    # Japan flood\n",
    "    # Typhoon Jebi\n",
    "    # Typhoon Mangkhut\n",
    "\n",
    "    path = DATA_PATH + folder_name\n",
    "    allFiles = glob.glob(path + \"\\\\*.csv\")\n",
    "\n",
    "    df_list = []\n",
    "    for file in allFiles:\n",
    "        df = pd.read_csv(file, header=0, engine='python').iloc[:, 1:]\n",
    "        df_list.append(df)\n",
    "\n",
    "\n",
    "    df = pd.concat(df_list).reset_index(drop=True)\n",
    "    df = df[['user','timestamp','text','likes','replies','retweets','tweet_id','url']]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define a function for text mining with the following steps:\n",
    "# # 1. remove the non English words\n",
    "# # 2. tokenize the string for each row\n",
    "# # 3. remove punctiation\n",
    "# # 4. convert each of the token to lower case\n",
    "# # 5. remove stopwords\n",
    "# # 6. lemmatize each of the token\n",
    "# # 7. join the tokens back into string\n",
    "\n",
    "# mystopwords = stopwords.words(\"English\")\n",
    "# wnlemma = nltk.WordNetLemmatizer()\n",
    "# def text_process(text):\n",
    "#     text = re.sub(r'\\d+', '', text)\n",
    "#     tokens = nltk.word_tokenize(text)\n",
    "#     tokens_nop = [word for word in tokens if word not in string.punctuation]\n",
    "#     tokens_lower = [ word.lower() for word in tokens_nop ]\n",
    "#     tokens_nostop = [word for word in tokens_lower if word not in mystopwords]\n",
    "#     tokens_lemma = [wnlemma.lemmatize(word) for word in tokens_nostop]\n",
    "#     text_after_process = \" \".join(tokens_lemma)\n",
    "#     return(text_after_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(dataframe, events):\n",
    "    df = dataframe.dropna(subset=['text'])\n",
    "    \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    df['date'] = df['timestamp'].dt.date\n",
    "    df['events'] = events\n",
    "#     df['text_processed'] = df['text'].apply(text_process)\n",
    "\n",
    "#     length = df['text'].apply(len)\n",
    "#     df = df.assign(length_text=length)\n",
    "\n",
    "#     length = df['text_processed'].apply(len)\n",
    "#     df = df.assign(length_text_processed=length)\n",
    "\n",
    "#     df = df.drop_duplicates(subset=['text'], keep='first')\n",
    "#     df = df.drop_duplicates(subset=['text_processed'], keep='first')\n",
    "    \n",
    "#     df = df[['user','timestamp','date','search_terms','text','text_processed','length_text','length_text_processed','likes','replies','retweets','tweet_id','url']]\n",
    "    df = df[['tweet_id','user','timestamp','date','events','text','likes','replies','retweets','url']]\n",
    "    df = df.sort_values(by = ['date'], ascending = True)\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(tweet):\n",
    "\n",
    "    wnlemma = nltk.WordNetLemmatizer()\n",
    "    printable = set(string.printable)\n",
    "\n",
    "    # remove non ASCII word\n",
    "    tweet = ''.join(filter(lambda x: x in printable, tweet))\n",
    "    \n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    \n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    \n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    \n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "\n",
    "    tweet = tweet.strip()\n",
    "    \n",
    "    tweet = re.sub(' +',' ', tweet)\n",
    "    \n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    " \n",
    "    tweets_clean = []    \n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and # remove stopwords\n",
    "              word not in emoticons and # remove emoticons\n",
    "                word not in string.punctuation): # remove punctuation\n",
    "            lemma_word = wnlemma.lemmatize(word)\n",
    "            tweets_clean.append(lemma_word)\n",
    "            \n",
    "    tweets_clean = \" \".join(tweets_clean)\n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleansing and Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Japan Floods 1\n",
    "# Japan Floods 2\n",
    "# Typhoon Jebi 1\n",
    "# Typhoon Jebi 2\n",
    "# Typhoon Mangkhut 1\n",
    "# Typhoon Mangkhut 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Japan_Floods_1 = import_data('Japan Floods 1')\n",
    "df_Japan_Floods_2 = import_data('Japan Floods 2')\n",
    "\n",
    "df_Typhoon_Jebi_1 = import_data('Typhoon Jebi 1')\n",
    "df_Typhoon_Jebi_2 = import_data('Typhoon Jebi 2')\n",
    "\n",
    "df_Typhoon_Mangkhut_1 = import_data('Typhoon Mangkhut 1')\n",
    "df_Typhoon_Mangkhut_2 = import_data('Typhoon Mangkhut 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df_Japan_Floods_1, df_Japan_Floods_2]\n",
    "df_Japan_Floods_full = pd.concat(frames)\n",
    "df_Japan_Floods_full = df_Japan_Floods_full.reset_index(drop=True)\n",
    "\n",
    "frames = [df_Typhoon_Jebi_1, df_Typhoon_Jebi_2]\n",
    "df_Typhoon_Jebi_full = pd.concat(frames)\n",
    "df_Typhoon_Jebi_full = df_Typhoon_Jebi_full.reset_index(drop=True)\n",
    "\n",
    "frames = [df_Typhoon_Mangkhut_1, df_Typhoon_Mangkhut_2]\n",
    "df_Typhoon_Mangkhut_full = pd.concat(frames)\n",
    "df_Typhoon_Mangkhut_full = df_Typhoon_Mangkhut_full.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 81335 entries, 0 to 81334\n",
      "Data columns (total 8 columns):\n",
      "user         81335 non-null object\n",
      "timestamp    81335 non-null object\n",
      "text         81335 non-null object\n",
      "likes        81335 non-null int64\n",
      "replies      81335 non-null int64\n",
      "retweets     81335 non-null int64\n",
      "tweet_id     81335 non-null int64\n",
      "url          81335 non-null object\n",
      "dtypes: int64(4), object(4)\n",
      "memory usage: 5.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df_Japan_Floods_full.info()\n",
    "# df_Japan_Floods_full = df_Japan_Floods_full[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 88160 entries, 0 to 88159\n",
      "Data columns (total 8 columns):\n",
      "user         88160 non-null object\n",
      "timestamp    88160 non-null object\n",
      "text         88160 non-null object\n",
      "likes        88160 non-null object\n",
      "replies      88160 non-null object\n",
      "retweets     88160 non-null object\n",
      "tweet_id     88160 non-null object\n",
      "url          88160 non-null object\n",
      "dtypes: object(8)\n",
      "memory usage: 5.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df_Typhoon_Jebi_full.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 71058 entries, 0 to 71057\n",
      "Data columns (total 8 columns):\n",
      "user         71050 non-null object\n",
      "timestamp    71050 non-null object\n",
      "text         71052 non-null object\n",
      "likes        71054 non-null float64\n",
      "replies      71054 non-null object\n",
      "retweets     71054 non-null object\n",
      "tweet_id     71050 non-null float64\n",
      "url          71050 non-null object\n",
      "dtypes: float64(2), object(6)\n",
      "memory usage: 4.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df_Typhoon_Mangkhut_full.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Japan_Floods_full = clean_data(df_Japan_Floods_full, 'Japan Floods')\n",
    "df_Japan_Floods_full = df_Japan_Floods_full.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Typhoon_Jebi_full = clean_data(df_Typhoon_Jebi_full, 'Typhoon Jebi')\n",
    "df_Typhoon_Jebi_full = df_Typhoon_Jebi_full.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\purpl\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\purpl\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\purpl\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df_Typhoon_Mangkhut_full = clean_data(df_Typhoon_Mangkhut_full, 'Typhoon Mangkhut')\n",
    "df_Typhoon_Mangkhut_full = df_Typhoon_Mangkhut_full.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\purpl\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:194: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "df_Japan_Floods_full['processed_text'] = \"\"\n",
    "for index, row in df_Japan_Floods_full.iterrows():\n",
    "    df_Japan_Floods_full['processed_text'].iloc[index] = clean_tweets(row['text'])\n",
    "df_Japan_Floods_full = df_Japan_Floods_full[['tweet_id','user','timestamp','date','events','text','processed_text','likes','replies','retweets','url']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\purpl\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:194: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "df_Typhoon_Jebi_full['processed_text'] = \"\"\n",
    "for index, row in df_Typhoon_Jebi_full.iterrows():\n",
    "    df_Typhoon_Jebi_full['processed_text'].iloc[index] = clean_tweets(row['text'])\n",
    "df_Typhoon_Jebi_full = df_Typhoon_Jebi_full[['tweet_id','user','timestamp','date','events','text','processed_text','likes','replies','retweets','url']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\purpl\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:194: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "df_Typhoon_Mangkhut_full['processed_text'] = \"\"\n",
    "for index, row in df_Typhoon_Mangkhut_full.iterrows():\n",
    "    df_Typhoon_Mangkhut_full['processed_text'].iloc[index] = clean_tweets(row['text'])\n",
    "df_Typhoon_Mangkhut_full = df_Typhoon_Mangkhut_full[['tweet_id','user','timestamp','date','events','text','processed_text','likes','replies','retweets','url']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>user</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>date</th>\n",
       "      <th>events</th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>likes</th>\n",
       "      <th>replies</th>\n",
       "      <th>retweets</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1005599354780958720</td>\n",
       "      <td>@Invesment_JpnJp</td>\n",
       "      <td>2018-06-09 23:55:43</td>\n",
       "      <td>2018-06-09</td>\n",
       "      <td>Japan Floods</td>\n",
       "      <td>We help you with Japan Tax Return at a reasona...</td>\n",
       "      <td>help japan tax return reasonable price minimum...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/Invesment_JpnJp/status/1005599354780958720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1005353978069209088</td>\n",
       "      <td>@TokyoAdultGuide</td>\n",
       "      <td>2018-06-09 07:40:41</td>\n",
       "      <td>2018-06-09</td>\n",
       "      <td>Japan Floods</td>\n",
       "      <td>New entry in Q&amp;A has been added... Choosing be...</td>\n",
       "      <td>new entry q added ... choosing threesome two h...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/TokyoAdultGuide/status/1005353978069209088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1005354512142553088</td>\n",
       "      <td>@KoltovskoyYakov</td>\n",
       "      <td>2018-06-09 07:42:48</td>\n",
       "      <td>2018-06-09</td>\n",
       "      <td>Japan Floods</td>\n",
       "      <td>Baghdad blasts kill 18 after recount law OKâ€™...</td>\n",
       "      <td>baghdad blast kill 18 recount law okd japan news</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/KoltovskoyYakov/status/1005354512142553088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1005354564428681216</td>\n",
       "      <td>@CherryMinus</td>\n",
       "      <td>2018-06-09 07:43:01</td>\n",
       "      <td>2018-06-09</td>\n",
       "      <td>Japan Floods</td>\n",
       "      <td>All this C94 preparation, I'm excited... Even ...</td>\n",
       "      <td>c94 preparation i'm excited ... even though i'...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/CherryMinus/status/1005354564428681216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1005355099541999619</td>\n",
       "      <td>@shinryutaishi</td>\n",
       "      <td>2018-06-09 07:45:08</td>\n",
       "      <td>2018-06-09</td>\n",
       "      <td>Japan Floods</td>\n",
       "      <td>Save our Nippon.\\nSave our Japan.\\nSave our co...</td>\n",
       "      <td>save nippon save japan save country themipn</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/shinryutaishi/status/1005355099541999619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id              user           timestamp        date  \\\n",
       "0  1005599354780958720  @Invesment_JpnJp 2018-06-09 23:55:43  2018-06-09   \n",
       "1  1005353978069209088  @TokyoAdultGuide 2018-06-09 07:40:41  2018-06-09   \n",
       "2  1005354512142553088  @KoltovskoyYakov 2018-06-09 07:42:48  2018-06-09   \n",
       "3  1005354564428681216      @CherryMinus 2018-06-09 07:43:01  2018-06-09   \n",
       "4  1005355099541999619    @shinryutaishi 2018-06-09 07:45:08  2018-06-09   \n",
       "\n",
       "         events                                               text  \\\n",
       "0  Japan Floods  We help you with Japan Tax Return at a reasona...   \n",
       "1  Japan Floods  New entry in Q&A has been added... Choosing be...   \n",
       "2  Japan Floods  Baghdad blasts kill 18 after recount law OKâ€™...   \n",
       "3  Japan Floods  All this C94 preparation, I'm excited... Even ...   \n",
       "4  Japan Floods  Save our Nippon.\\nSave our Japan.\\nSave our co...   \n",
       "\n",
       "                                      processed_text  likes  replies  \\\n",
       "0  help japan tax return reasonable price minimum...      0        0   \n",
       "1  new entry q added ... choosing threesome two h...      1        0   \n",
       "2   baghdad blast kill 18 recount law okd japan news      0        0   \n",
       "3  c94 preparation i'm excited ... even though i'...      0        0   \n",
       "4        save nippon save japan save country themipn      0        0   \n",
       "\n",
       "   retweets                                          url  \n",
       "0         0  /Invesment_JpnJp/status/1005599354780958720  \n",
       "1         0  /TokyoAdultGuide/status/1005353978069209088  \n",
       "2         0  /KoltovskoyYakov/status/1005354512142553088  \n",
       "3         0      /CherryMinus/status/1005354564428681216  \n",
       "4         0    /shinryutaishi/status/1005355099541999619  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Japan_Floods_full.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>user</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>date</th>\n",
       "      <th>events</th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>likes</th>\n",
       "      <th>replies</th>\n",
       "      <th>retweets</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1022632341351669760</td>\n",
       "      <td>@EqualizerSoccer</td>\n",
       "      <td>2018-07-26 23:58:44</td>\n",
       "      <td>2018-07-26</td>\n",
       "      <td>Typhoon Jebi</td>\n",
       "      <td>42' - Ertz gets wide open inside Japan's box b...</td>\n",
       "      <td>42 ertz get wide open inside japan's box keepe...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>/EqualizerSoccer/status/1022632341351669760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1022400668013744128</td>\n",
       "      <td>@pedromj</td>\n",
       "      <td>2018-07-26 08:38:09</td>\n",
       "      <td>2018-07-26</td>\n",
       "      <td>Typhoon Jebi</td>\n",
       "      <td>Typhoon Jongdari remains on course to hit Japa...</td>\n",
       "      <td>typhoon jongdari remains course hit japan's ma...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/pedromj/status/1022400668013744128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1022400406066872320</td>\n",
       "      <td>@tokyostyle_no1</td>\n",
       "      <td>2018-07-26 08:37:06</td>\n",
       "      <td>2018-07-26</td>\n",
       "      <td>Typhoon Jebi</td>\n",
       "      <td>I'm Satomi\\nYou can shake off the fatigue of t...</td>\n",
       "      <td>i'm satomi shake fatigue traveling japan happy...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/tokyostyle_no1/status/1022400406066872320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1022400138122211328</td>\n",
       "      <td>@HongKongFP</td>\n",
       "      <td>2018-07-26 08:36:02</td>\n",
       "      <td>2018-07-26</td>\n",
       "      <td>Typhoon Jebi</td>\n",
       "      <td>Palau seeks help from US and Japan to counter ...</td>\n",
       "      <td>palau seek help u japan counter china's touris...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>/HongKongFP/status/1022400138122211328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1022399914645639173</td>\n",
       "      <td>@dwnreport</td>\n",
       "      <td>2018-07-26 08:35:09</td>\n",
       "      <td>2018-07-26</td>\n",
       "      <td>Typhoon Jebi</td>\n",
       "      <td>Palau asks US and Japan for help after China i...</td>\n",
       "      <td>palau asks u japan help china imposes tourist ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/dwnreport/status/1022399914645639173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id              user           timestamp        date  \\\n",
       "0  1022632341351669760  @EqualizerSoccer 2018-07-26 23:58:44  2018-07-26   \n",
       "1  1022400668013744128          @pedromj 2018-07-26 08:38:09  2018-07-26   \n",
       "2  1022400406066872320   @tokyostyle_no1 2018-07-26 08:37:06  2018-07-26   \n",
       "3  1022400138122211328       @HongKongFP 2018-07-26 08:36:02  2018-07-26   \n",
       "4  1022399914645639173        @dwnreport 2018-07-26 08:35:09  2018-07-26   \n",
       "\n",
       "         events                                               text  \\\n",
       "0  Typhoon Jebi  42' - Ertz gets wide open inside Japan's box b...   \n",
       "1  Typhoon Jebi  Typhoon Jongdari remains on course to hit Japa...   \n",
       "2  Typhoon Jebi  I'm Satomi\\nYou can shake off the fatigue of t...   \n",
       "3  Typhoon Jebi  Palau seeks help from US and Japan to counter ...   \n",
       "4  Typhoon Jebi  Palau asks US and Japan for help after China i...   \n",
       "\n",
       "                                      processed_text likes replies retweets  \\\n",
       "0  42 ertz get wide open inside japan's box keepe...     7       0        1   \n",
       "1  typhoon jongdari remains course hit japan's ma...     0       0        0   \n",
       "2  i'm satomi shake fatigue traveling japan happy...     0       0        0   \n",
       "3  palau seek help u japan counter china's touris...     5       1        3   \n",
       "4  palau asks u japan help china imposes tourist ...     0       0        0   \n",
       "\n",
       "                                           url  \n",
       "0  /EqualizerSoccer/status/1022632341351669760  \n",
       "1          /pedromj/status/1022400668013744128  \n",
       "2   /tokyostyle_no1/status/1022400406066872320  \n",
       "3       /HongKongFP/status/1022400138122211328  \n",
       "4        /dwnreport/status/1022399914645639173  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Typhoon_Jebi_full.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>user</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>date</th>\n",
       "      <th>events</th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>likes</th>\n",
       "      <th>replies</th>\n",
       "      <th>retweets</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.026975e+18</td>\n",
       "      <td>@Kuwago68</td>\n",
       "      <td>2018-08-07 23:35:23</td>\n",
       "      <td>2018-08-07</td>\n",
       "      <td>Typhoon Mangkhut</td>\n",
       "      <td>Ummm...Well...Ummm...To Late. He'd already mad...</td>\n",
       "      <td>ummm ... well ... ummm ... late he'd already m...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>/Kuwago68/status/1026975117950107648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.026760e+18</td>\n",
       "      <td>@SCMPNews</td>\n",
       "      <td>2018-08-07 09:20:03</td>\n",
       "      <td>2018-08-07</td>\n",
       "      <td>Typhoon Mangkhut</td>\n",
       "      <td>Crowdfunding to help needy Hong Kong cancer pa...</td>\n",
       "      <td>crowdfunding help needy hong kong cancer patie...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>/SCMPNews/status/1026759867103621120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.026759e+18</td>\n",
       "      <td>@jtmama</td>\n",
       "      <td>2018-08-07 09:17:10</td>\n",
       "      <td>2018-08-07</td>\n",
       "      <td>Typhoon Mangkhut</td>\n",
       "      <td>U gent help needed..... please contact either ...</td>\n",
       "      <td>u gent help needed ... please contact either u...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/jtmama/status/1026759142696841216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.026756e+18</td>\n",
       "      <td>@fidel_hon</td>\n",
       "      <td>2018-08-07 09:04:41</td>\n",
       "      <td>2018-08-07</td>\n",
       "      <td>Typhoon Mangkhut</td>\n",
       "      <td>Im from Bloody Hong Kong so i should kill myself</td>\n",
       "      <td>im bloody hong kong kill</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/fidel_hon/status/1026755999363883008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.026746e+18</td>\n",
       "      <td>@IFRAsia</td>\n",
       "      <td>2018-08-07 08:25:11</td>\n",
       "      <td>2018-08-07</td>\n",
       "      <td>Typhoon Mangkhut</td>\n",
       "      <td>China Everbright Water seeks dual primary list...</td>\n",
       "      <td>china everbright water seek dual primary listi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/IFRAsia/status/1026746061023535104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tweet_id        user           timestamp        date            events  \\\n",
       "0  1.026975e+18   @Kuwago68 2018-08-07 23:35:23  2018-08-07  Typhoon Mangkhut   \n",
       "1  1.026760e+18   @SCMPNews 2018-08-07 09:20:03  2018-08-07  Typhoon Mangkhut   \n",
       "2  1.026759e+18     @jtmama 2018-08-07 09:17:10  2018-08-07  Typhoon Mangkhut   \n",
       "3  1.026756e+18  @fidel_hon 2018-08-07 09:04:41  2018-08-07  Typhoon Mangkhut   \n",
       "4  1.026746e+18    @IFRAsia 2018-08-07 08:25:11  2018-08-07  Typhoon Mangkhut   \n",
       "\n",
       "                                                text  \\\n",
       "0  Ummm...Well...Ummm...To Late. He'd already mad...   \n",
       "1  Crowdfunding to help needy Hong Kong cancer pa...   \n",
       "2  U gent help needed..... please contact either ...   \n",
       "3  Im from Bloody Hong Kong so i should kill myself    \n",
       "4  China Everbright Water seeks dual primary list...   \n",
       "\n",
       "                                      processed_text  likes replies retweets  \\\n",
       "0  ummm ... well ... ummm ... late he'd already m...    1.0       1        0   \n",
       "1  crowdfunding help needy hong kong cancer patie...    3.0       0        2   \n",
       "2  u gent help needed ... please contact either u...    0.0       0        0   \n",
       "3                           im bloody hong kong kill    0.0       0        0   \n",
       "4  china everbright water seek dual primary listi...    0.0       0        0   \n",
       "\n",
       "                                     url  \n",
       "0   /Kuwago68/status/1026975117950107648  \n",
       "1   /SCMPNews/status/1026759867103621120  \n",
       "2     /jtmama/status/1026759142696841216  \n",
       "3  /fidel_hon/status/1026755999363883008  \n",
       "4    /IFRAsia/status/1026746061023535104  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Typhoon_Mangkhut_full.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Japan_Floods_full.to_csv(DATA_PATH + \"\\\\Full\\\\df_Japan_Floods_full.csv\")\n",
    "df_Typhoon_Jebi_full.to_csv(DATA_PATH + \"\\\\Full\\\\df_Typhoon_Jebi_full.csv\")\n",
    "df_Typhoon_Mangkhut_full.to_csv(DATA_PATH + \"\\\\Full\\\\df_Typhoon_Mangkhut_full.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
